{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Part-I-–-Optimisation\" data-toc-modified-id=\"Part-I-–-Optimisation-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Part I – Optimisation</a></span><ul class=\"toc-item\"><li><span><a href=\"#Topics-Covered\" data-toc-modified-id=\"Topics-Covered-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Topics Covered</a></span></li><li><span><a href=\"#Topic-Notes\" data-toc-modified-id=\"Topic-Notes-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Topic Notes</a></span></li><li><span><a href=\"#Workflow-and-Assessment-\" data-toc-modified-id=\"Workflow-and-Assessment--1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>Workflow and Assessment <a name=\"WorkflowandAssessment\" rel=\"nofollow\"></a></a></span><ul class=\"toc-item\"><li><span><a href=\"#Objectives:-\" data-toc-modified-id=\"Objectives:--1.3.1\"><span class=\"toc-item-num\">1.3.1&nbsp;&nbsp;</span>Objectives: <a name=\"Objectives\" rel=\"nofollow\"></a></a></span><ul class=\"toc-item\"><li><span><a href=\"#Common-objectives-of-all-workshops-\" data-toc-modified-id=\"Common-objectives-of-all-workshops--1.3.1.1\"><span class=\"toc-item-num\">1.3.1.1&nbsp;&nbsp;</span>Common objectives of all workshops <a name=\"Commonobjectivesofallworkshops\" rel=\"nofollow\"></a></a></span></li></ul></li><li><span><a href=\"#Assessment-Process-\" data-toc-modified-id=\"Assessment-Process--1.3.2\"><span class=\"toc-item-num\">1.3.2&nbsp;&nbsp;</span>Assessment Process <a name=\"AssessmentProcess\" rel=\"nofollow\"></a></a></span></li></ul></li><li><span><a href=\"#Convex-Functions-\" data-toc-modified-id=\"Convex-Functions--1.4\"><span class=\"toc-item-num\">1.4&nbsp;&nbsp;</span>Convex Functions <a name=\"SectionConvexFunctions\" rel=\"nofollow\"></a></a></span><ul class=\"toc-item\"><li><span><a href=\"#Practice-Question\" data-toc-modified-id=\"Practice-Question-1.4.1\"><span class=\"toc-item-num\">1.4.1&nbsp;&nbsp;</span>Practice Question</a></span></li><li><span><a href=\"#Curiousity-Question\" data-toc-modified-id=\"Curiousity-Question-1.4.2\"><span class=\"toc-item-num\">1.4.2&nbsp;&nbsp;</span>Curiousity Question</a></span></li><li><span><a href=\"#Discussion-Question\" data-toc-modified-id=\"Discussion-Question-1.4.3\"><span class=\"toc-item-num\">1.4.3&nbsp;&nbsp;</span>Discussion Question</a></span></li></ul></li><li><span><a href=\"#Unconstrained-Optimisation-\" data-toc-modified-id=\"Unconstrained-Optimisation--1.5\"><span class=\"toc-item-num\">1.5&nbsp;&nbsp;</span>Unconstrained Optimisation <a name=\"SectionUnconstrainedOptimisation\" rel=\"nofollow\"></a></a></span><ul class=\"toc-item\"><li><span><a href=\"#Example:-Aloha-communication-protocol-\" data-toc-modified-id=\"Example:-Aloha-communication-protocol--1.5.1\"><span class=\"toc-item-num\">1.5.1&nbsp;&nbsp;</span>Example: Aloha communication protocol <a name=\"ExampleAlohacommunicationprotocol\" rel=\"nofollow\"></a></a></span></li><li><span><a href=\"#Slotted-Aloha-Efficiency-\" data-toc-modified-id=\"Slotted-Aloha-Efficiency--1.5.2\"><span class=\"toc-item-num\">1.5.2&nbsp;&nbsp;</span>Slotted Aloha Efficiency <a name=\"SlottedAlohaEfficiency\" rel=\"nofollow\"></a></a></span></li><li><span><a href=\"#Question\" data-toc-modified-id=\"Question-1.5.3\"><span class=\"toc-item-num\">1.5.3&nbsp;&nbsp;</span>Question</a></span></li><li><span><a href=\"#Question\" data-toc-modified-id=\"Question-1.5.4\"><span class=\"toc-item-num\">1.5.4&nbsp;&nbsp;</span>Question</a></span></li></ul></li><li><span><a href=\"#Constrained-Optimisation-\" data-toc-modified-id=\"Constrained-Optimisation--1.6\"><span class=\"toc-item-num\">1.6&nbsp;&nbsp;</span>Constrained Optimisation <a name=\"SectionConstrainedOptimisation\" rel=\"nofollow\"></a></a></span></li><li><span><a href=\"#Example:-Non-Convex-Optimisation\" data-toc-modified-id=\"Example:-Non-Convex-Optimisation-1.7\"><span class=\"toc-item-num\">1.7&nbsp;&nbsp;</span>Example: Non-Convex Optimisation</a></span></li><li><span><a href=\"#Using-random.seed()-for-pseudo-random-number-generation-\" data-toc-modified-id=\"Using-random.seed()-for-pseudo-random-number-generation--1.8\"><span class=\"toc-item-num\">1.8&nbsp;&nbsp;</span>Using random.seed() for pseudo-random number generation <a name=\"Usingrandomseedforpseudorandomnumbergeneration\" rel=\"nofollow\"></a></a></span><ul class=\"toc-item\"><li><span><a href=\"#Important-Note-on-Random-Number/Vector-Generation-\" data-toc-modified-id=\"Important-Note-on-Random-Number/Vector-Generation--1.8.1\"><span class=\"toc-item-num\">1.8.1&nbsp;&nbsp;</span>Important Note on Random Number/Vector Generation <a name=\"ImportantNoteonRandomNumberVectorGeneration\" rel=\"nofollow\"></a></a></span></li></ul></li><li><span><a href=\"#Example-Economic-Dispatch-in-Power-Generation\" data-toc-modified-id=\"Example-Economic-Dispatch-in-Power-Generation-1.9\"><span class=\"toc-item-num\">1.9&nbsp;&nbsp;</span>Example Economic Dispatch in Power Generation</a></span><ul class=\"toc-item\"><li><span><a href=\"#Question:-Economic-Dispatch\" data-toc-modified-id=\"Question:-Economic-Dispatch-1.9.1\"><span class=\"toc-item-num\">1.9.1&nbsp;&nbsp;</span>Question: Economic Dispatch</a></span></li></ul></li><li><span><a href=\"#Example:-Waterfilling-in-Communications\" data-toc-modified-id=\"Example:-Waterfilling-in-Communications-1.10\"><span class=\"toc-item-num\">1.10&nbsp;&nbsp;</span>Example: Waterfilling in Communications</a></span><ul class=\"toc-item\"><li><span><a href=\"#Question\" data-toc-modified-id=\"Question-1.10.1\"><span class=\"toc-item-num\">1.10.1&nbsp;&nbsp;</span>Question</a></span></li></ul></li><li><span><a href=\"#Example:-Power-Control-in-Wireless-Communication\" data-toc-modified-id=\"Example:-Power-Control-in-Wireless-Communication-1.11\"><span class=\"toc-item-num\">1.11&nbsp;&nbsp;</span>Example: Power Control in Wireless Communication</a></span><ul class=\"toc-item\"><li><span><a href=\"#Question:-Wireless-Power-Control\" data-toc-modified-id=\"Question:-Wireless-Power-Control-1.11.1\"><span class=\"toc-item-num\">1.11.1&nbsp;&nbsp;</span>Question: Wireless Power Control</a></span></li></ul></li></ul></li><li><span><a href=\"#Part-II---Machine-Learning-(ML)\" data-toc-modified-id=\"Part-II---Machine-Learning-(ML)-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Part II - Machine Learning (ML)</a></span><ul class=\"toc-item\"><li><span><a href=\"#Topics-Covered-in-Part-2\" data-toc-modified-id=\"Topics-Covered-in-Part-2-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Topics Covered in Part 2<a name=\"TopicsCovered\" rel=\"nofollow\"></a></a></span></li><li><span><a href=\"#Part-2-Notes--\" data-toc-modified-id=\"Part-2-Notes---2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Part 2 Notes  <a name=\"TopicNotes\" rel=\"nofollow\"></a></a></span></li><li><span><a href=\"#Part-2-Objectives-\" data-toc-modified-id=\"Part-2-Objectives--2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>Part 2 Objectives <a name=\"Objectives\" rel=\"nofollow\"></a></a></span></li><li><span><a href=\"#Linear-Regression,-Overfitting,-and-Regularisation\" data-toc-modified-id=\"Linear-Regression,-Overfitting,-and-Regularisation-2.4\"><span class=\"toc-item-num\">2.4&nbsp;&nbsp;</span>Linear Regression, Overfitting, and Regularisation</a></span><ul class=\"toc-item\"><li><span><a href=\"#Example:-Using-curve-fitting-to-model-Diode-characteristics\" data-toc-modified-id=\"Example:-Using-curve-fitting-to-model-Diode-characteristics-2.4.1\"><span class=\"toc-item-num\">2.4.1&nbsp;&nbsp;</span>Example: Using curve fitting to model Diode characteristics</a></span></li><li><span><a href=\"#Question:-Regression\" data-toc-modified-id=\"Question:-Regression-2.4.2\"><span class=\"toc-item-num\">2.4.2&nbsp;&nbsp;</span>Question: Regression</a></span><ul class=\"toc-item\"><li><span><a href=\"#Hints-\" data-toc-modified-id=\"Hints--2.4.2.1\"><span class=\"toc-item-num\">2.4.2.1&nbsp;&nbsp;</span>Hints <a name=\"Hints\" rel=\"nofollow\"></a></a></span></li></ul></li></ul></li><li><span><a href=\"#Clustering-and-Gaussian-Mixtures\" data-toc-modified-id=\"Clustering-and-Gaussian-Mixtures-2.5\"><span class=\"toc-item-num\">2.5&nbsp;&nbsp;</span>Clustering and Gaussian Mixtures</a></span><ul class=\"toc-item\"><li><span><a href=\"#Question:-K-means-clustering\" data-toc-modified-id=\"Question:-K-means-clustering-2.5.1\"><span class=\"toc-item-num\">2.5.1&nbsp;&nbsp;</span>Question: K-means clustering</a></span></li><li><span><a href=\"#Question:-GMMs-as-density-estimators\" data-toc-modified-id=\"Question:-GMMs-as-density-estimators-2.5.2\"><span class=\"toc-item-num\">2.5.2&nbsp;&nbsp;</span>Question: GMMs as density estimators</a></span></li></ul></li></ul></li><li><span><a href=\"#-Workshop-Assessment-Instructions-\" data-toc-modified-id=\"-Workshop-Assessment-Instructions--3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span><font color=\"red\"> Workshop Assessment Instructions </font></a></span><ul class=\"toc-item\"><li><span><a href=\"#Workshop-Marking\" data-toc-modified-id=\"Workshop-Marking-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Workshop Marking</a></span><ul class=\"toc-item\"><li><span><a href=\"#Additional-guidelines-for-your-programs:\" data-toc-modified-id=\"Additional-guidelines-for-your-programs:-3.1.1\"><span class=\"toc-item-num\">3.1.1&nbsp;&nbsp;</span>Additional guidelines for your programs:</a></span></li></ul></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size:2em\">Workshop 1 – Optimisation and Machine Learning [3 weeks] </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part I – Optimisation\n",
    "\n",
    "## Topics Covered \n",
    "    \n",
    "* Optimisation basics, formulation of optimisation problems\n",
    "* Local/global minima/maxima\n",
    "* Convex sets and functions\n",
    "* Unconstrained optimisation and optimality conditions\n",
    "* Constrained optimisation, equality and inequality constraints, constraint sets\n",
    "* Lagrange theory and KKT conditions\n",
    "     \n",
    "![Topic_mindmap](img/WS1-Part_1-topics.png)     \n",
    "\n",
    "\n",
    "## Topic Notes  \n",
    "Another name for the field of “Optimisation” is “Mathematical Optimisation.” As the name indicates, optimisation is an area of applied mathematics. It is possible to study optimisation entirely from a mathematical perspective. However, engineers are interested in solving real-world problems in a principled way. Many engineering problems can be and are formulated as optimisation problems. In those cases, mathematical optimisation provides a solid theoretical foundation for solving them in a principled way.\n",
    "\n",
    "In this workshop, you will learn how to formulate and solve optimisation problems in practice. This will give you a chance to connect theoretical knowledge and practical usage by doing it yourself. You will familiarise yourself with practical optimisation tools for Python. These are chosen completely for educational reasons (simplicity, accessibility, cost). While the underlying mathematics is timeless, optimisation software evolves with time, and can be diverse. Fortunately, once you learn one or two, it should be rather easy to learn others now and in the future because software designers often try to make it user friendly and take into account what people already know. \n",
    "\n",
    "> In the future, you should consider learning serious [optimisation software](https://en.wikipedia.org/wiki/List_of_optimization_software) for scalability and reliability. They can be complex and/or expensive but they get the job done for serious engineering. Learning such software takes a significant amount of time and is beyond the scope of this subject."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Workflow and Assessment <a name=\"WorkflowandAssessment\"></a>\n",
    "\n",
    "This subject follows a problem- and project-oriented approach. In this learning workflow, the focus is on solving practical (engineering) problems, which motivate acquiring theoretical (background) knowledge at the same time.\n",
    "\n",
    "### Objectives: <a name=\"Objectives\"></a>\n",
    "* Use these problems as a motivation to learn the fundamentals of optimisation covered in lectures.\n",
    "* Learn how to formulate and solve optimisation problems in practice.\n",
    "* Familiarise yourself with practical software tools used for optimisation.\n",
    "* Solve optimisation problems using Python (and/or Matlab).\n",
    "* Connect theoretical knowledge and practical usage by doing it yourself.\n",
    "#### Common objectives of all workshops <a name=\"Commonobjectivesofallworkshops\"></a>\n",
    "Gain hands-on experience and learn by doing! Understand how theoretical knowledge discussed in lectures relates to practice. Develop motivation for gaining further theoretical and practical knowledge beyond the subject material.\n",
    "\n",
    "> __Self-learning__ is one of the most important skills that you should acquire as a student. Today, self-learning is much easier than it used to be thanks to a plethora of online resources.\n",
    "\n",
    "### Assessment Process <a name=\"AssessmentProcess\"></a>\n",
    "\n",
    "1. Follow the procedures described below, perform the given tasks, and answer the workshop questions __in this Python notebook itself! The resulting notebook will be your Workshop Report!__\n",
    "2. Submit the workshop report at the announced deadline\n",
    "3. Demonstrators will conduct a brief (5min) oral quiz on your submitted report in the subsequent weeks.\n",
    "4. Your workshop marks will be a combination of the report you submitted and oral quiz results.\n",
    "\n",
    "\n",
    "> __The goal is to learn__, NOT blindly follow the procedures in the fastest possible way! __Do not simply copy-paste answers (from Internet, friends, etc.). You can and should use all available resources but only to develop your own understanding. If you copy-paste, you will pay the price in the oral quiz!__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Convex Functions <a name=\"SectionConvexFunctions\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember the definition of convex and concave functions from lecture slides. Functions are mathematical objects but they are used in engineering in very practical ways, for example, to represent the relationship between two quantities. Let's draw a function!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# define function f(x)\n",
    "def f(x):\n",
    "    return 5*(x-1)**2\n",
    "\n",
    "# define x and y\n",
    "x = np.linspace(-20, 20, 100) # 100 equally spaced points on interval [-20,20]\n",
    "y = f(x) # call function f(x) and set y to the function's return value\n",
    "\n",
    "# Plot the function y=f(x)\n",
    "plt.plot(x,y)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.title('$y=5(x-1)^2$')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Practice Question\n",
    "Plot one concave and one non-concave function of your choosing (preferably one in 2 dimensions and the other in 3 dimensions so that it can be visualised, check e.g. [this tutorial](https://matplotlib.org/2.0.2/mpl_toolkits/mplot3d/tutorial.html) for hints). Provide their formulas below.\n",
    "\n",
    "*Hint: an interesting and well-known function is [Rosenbrock function].(https://en.wikipedia.org/wiki/Rosenbrock_function)* It is already built-in to [Scipy optimize](https://docs.scipy.org/doc/scipy/reference/optimize.html#benchmark-problems) as a benchmark. *You can keep your answer simple and don't need to spend too much time on this practice question.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Answer as code here '''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer as text here**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Curiousity Question \n",
    "How would you determine whether a single or multi-variate continuously differentiable function is convex or not? \n",
    "> Note that the question becomes very tricky if you have a **parametric** multivariate polynomial of degree four or higher! \n",
    "\n",
    "> *[Optional]* An interesting paper (for those who wish to go deeper) http://web.mit.edu/~a_a_a/Public/Publications/convexity_nphard.pdf "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer as text here**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion Question \n",
    "Why are convex optimisation problems considered to be easy to solve? Consider optimality conditions of unconstrained functions in your answer. Plot the first- and second-order derivative functions for one concave and one non-concave function (this time only in 2 dimensions) to further support your argument."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer as text here**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Answer as code here '''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unconstrained Optimisation <a name=\"SectionUnconstrainedOptimisation\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Aloha communication protocol <a name=\"ExampleAlohacommunicationprotocol\"></a>\n",
    "\n",
    "![Wireless Network](img/wireless.jpg)\n",
    "\n",
    "**Aloha** is a well-known random access or _MAC_ (Media/multiple Access Control) communication protocol. It enables multiple nodes to share a broadcast channel without any additional signaling in a distributed manner. Unlike _FDMA_ or _TDMA_ (frequency or time-division multiple access), the channel is not divided into segments beforehand and collisions of packets due to simultaneous transmissions by nodes are allowed. In slotted Aloha, the nodes can only transmit at the beginning of time slots, which are kept by a global/shared clock. See [Aloha](https://en.wikipedia.org/wiki/ALOHAnet#Slotted_ALOHA) for further background information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slotted Aloha Efficiency <a name=\"SlottedAlohaEfficiency\"></a>\n",
    "\n",
    "For an $N$-node slotted Aloha system, where each node transmits with a probability $p$, the throughput of the system is given by\n",
    "$$ S(p) = N p (1-p)^{N-1}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question \n",
    "Formally define the optimisation problem to find the optimal probability $p$ that maximises the throughput. Clearly identify the objective and decision variable(s). Is the objective convex or concave? Show through derivation. Find the optimality conditions for this problem. \n",
    "\n",
    "Note that there is the constraint $0 \\leq p \\leq 1$ on probability $p$ but we will ignore it for now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer as text here**  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer as text here**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Answer as code here '''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question \n",
    "Find the optimal probability $p$ for $N=12$ nodes. Use an appropriate package from [Scipy](https://docs.scipy.org/doc/scipy/reference/optimize.html). Cross-check your answer with a mathematical formula that you should derive by hand. \n",
    "\n",
    "*Hint: see [examples and documentation for scalar case.](https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.minimize_scalar.html#scipy.optimize.minimize_scalar)*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import optimize\n",
    "''' Answer as code here '''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer as text here**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constrained Optimisation <a name=\"SectionConstrainedOptimisation\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Pyomo](http://www.pyomo.org/) is a Python-based tool for modeling and solving optimisation problems. Algebraic modeling languages (AMLs) like Pyomo are high-level languages for specifying and solving mathematical optimisation problems. Widely used commercial AMLs include AIMMS, AMPL, and GAMS. \n",
    "\n",
    "Pyomo uses the following mathematical concepts that are central to modern modeling activities:\n",
    "* Variables: These represent unknown or changing parts of a model (e.g., which decisions to take, or the characteristic of a system outcome).\n",
    "* Parameters: These are symbolic representations for real-world data, which might vary for different problem instances or scenarios.\n",
    "* Relations: These are equations, inequalities, or other mathematical relationships that define how different parts of a model are related to each other.\n",
    "\n",
    "Pyomo supports an object-oriented design for the definition of optimisation models.\n",
    "A Pyomo model object contains a collection of modeling components that define the optimisation problem. The Pyomo package includes modeling components that are necessary to formulate an optimisation problem: variables, objectives, and constraints, as well as other modeling components that are commonly supported by modern AMLs, including index sets and parameters.\n",
    "\n",
    "The [pyomo online documentation](https://pyomo.readthedocs.io/en/stable/index.html) gives you an excellent starting point. There is also an entire [book](https://www.springer.com/gp/book/9783319588193) for those who are interested.  \n",
    "\n",
    "**See the instructions to install Pyomo, based on [Pyomo's instructions using conda](https://pyomo.readthedocs.io/en/stable/installation.html#using-conda).**\n",
    "\n",
    "> **2021 Installation Problem for Windows:** Ipopt installation is a bit broken under Windows. All you need to do is copy the  ipopt.exe we have provided in files (originally from [pre-compiled sources, 3.11.1 64 or 32 bit version](https://www.coin-or.org/download/binary/Ipopt/)) and copy that to Anaconda's `\\Library\\bin` directory. This directory can be found using the command `where conda`. After copying ipopt.exe, close and reopen anaconda (and this notebook) before continuing.\n",
    "\n",
    "*Hint: to process Pyomo results, see [working with Pyomo models](https://pyomo.readthedocs.io/en/stable/working_models.html), e.g. you can [access Lagrange multipliers (duals)](https://pyomo.readthedocs.io/en/stable/working_models.html#accessing-duals) by passing an argument to solver.*\n",
    "\n",
    "**Suggested exercise:** verify your answer to Aloha optimisation above by formulating that problem as a Pyomo model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Non-Convex Optimisation\n",
    "\n",
    "The [Rosenbrock function](https://en.wikipedia.org/wiki/Rosenbrock_function) is a non-convex function, introduced by Howard H. Rosenbrock in 1960, which is used as a performance test problem for global optimisation algorithms. A two-variable, arbitrarily-constrained variant is\n",
    "\n",
    "$$ \\min_{x \\in \\mathcal A} f(x) = (1 − x_1)^2 + 100(x_2 -x_1^2)^2 ,$$\n",
    "where the constraint set is defined by\n",
    "$$ \\mathcal A := \\{ x \\in \\mathbb R^2 | x_2\\geq x_1+1,\\, x_1 \\in [-2, 3], x_2 \\in [-2, 2] \\}.$$\n",
    "\n",
    "Let us use the following abstract [Pyomo model](https://pyomo.readthedocs.io/en/stable/pyomo_overview/simple_examples.html) for this problem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A Pyomo model for the Rosenbrock problem\n",
    "import pyomo.environ as pyo\n",
    "from pyomo.opt import SolverFactory\n",
    "\n",
    "model = pyo.AbstractModel()\n",
    "model.name = 'Rosenbrock'\n",
    "\n",
    "# note boundaries of variables and initial condition $x_0=[-2,2]$\n",
    "model.x1 = pyo.Var(bounds=(-2,3), initialize=-2)\n",
    "model.x2 = pyo.Var(bounds=(-2,2), initialize=2)\n",
    "\n",
    "def rosenbrock(model):\n",
    "    f = (1.0-model.x1)**2 + 100.0*(model.x2 - model.x1**2)**2\n",
    "    return f\n",
    "\n",
    "def ineqconstr(model):\n",
    "    return model.x2 >= model.x1+1\n",
    "\n",
    "model.obj = pyo.Objective(rule=rosenbrock, sense=pyo.minimize)\n",
    "model.constraint = pyo.Constraint(rule=ineqconstr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model can be solved in multiple different ways. Since this is a Pyomo AbstractModel, we must create a concrete instance of it before solving. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    " \n",
    "# declare a dummy file\n",
    "dummy_file = io.StringIO() #we create a in-memory text stream\n",
    "\n",
    "# create an instance of the problem\n",
    "arosenbrockproblem = model.create_instance()\n",
    "# this is to access Lagrange multipliers (dual variables)\n",
    "arosenbrockproblem.dual = pyo.Suffix(direction=pyo.Suffix.IMPORT)\n",
    "\n",
    "# define solver\n",
    "opt = pyo.SolverFactory('ipopt') # we can use other solvers here as well\n",
    "\n",
    "results = opt.solve(arosenbrockproblem, tee=True, logfile = \"name.csv\") \n",
    "\n",
    "# show results\n",
    "\n",
    "arosenbrockproblem.display(ostream=dummy_file)\n",
    "#arosenbrockproblem.display(filename=\"output_file\")  #alternatively write output to a file\n",
    "#help(arosenbrockproblem.display)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_file.getvalue()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arosenbrockproblem.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's see the Lagrange multipliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_lagrange(instance):\n",
    "    # display all duals\n",
    "    print (\"Duals\")\n",
    "    for c in instance.component_objects(pyo.Constraint, active=True):\n",
    "        print (\"   Constraint\",c)\n",
    "        for index in c:\n",
    "            print (\"      \", index, instance.dual[c[index]])\n",
    "            \n",
    "display_lagrange(arosenbrockproblem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display the solution directly "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def disp_soln(instance):\n",
    "    output = []\n",
    "    for v in instance.component_data_objects(pyo.Var, active=True):\n",
    "        output.append(pyo.value(v))\n",
    "        print(v, pyo.value(v))  \n",
    "    print (instance.obj, pyo.value(instance.obj))\n",
    "    output.append(pyo.value(instance.obj))\n",
    "    return output\n",
    "\n",
    "            \n",
    "disp_soln(arosenbrockproblem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since this is a non-convex optimisation problem, the solver can only find local solutions! What happens if we change the starting point to $x_0=[1.5, 1.5]$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arosenbrockproblem.x1 = 1.5\n",
    "arosenbrockproblem.x2 = 1.5\n",
    "\n",
    "results = opt.solve(arosenbrockproblem) \n",
    "\n",
    "arosenbrockproblem.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_lagrange(arosenbrockproblem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disp_soln(arosenbrockproblem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir(arosenbrockproblem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using random.seed() for pseudo-random number generation <a name=\"Usingrandomseedforpseudorandomnumbergeneration\"></a>\n",
    "\n",
    "We use Python or numpy random modules to generate random numbers or vectors/matrices. Random numbers and vectors are used a lot in various context, e.g. as asked above or in machine learning problems. They also cause a *reproducibility* problem. In this case, when your demonstrators run your code, they will get a different $\\alpha$ vector each time and the results will be different each time. How can we prevent that? Python *random.seed* addresses this problem. \n",
    "\n",
    "If you call *random.seed(seedvalue)* every time before calling the random number generator, you will get the same \"random\" number. Change the seed and the number will change as well. This makes it [pseudo-random](https://en.wikipedia.org/wiki/Pseudorandomness) which ensures reproducibility.\n",
    "\n",
    "[See this nice article for basics](https://pynative.com/python-random-seed/). To generate random vectors and matrices [numpy](https://docs.scipy.org/doc/numpy-1.15.1/reference/generated/numpy.random.seed.html#numpy.random.seed) is very useful. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "print (\"Random numbers with a given seed\")\n",
    "random.seed(1215151)\n",
    "print (random.random())\n",
    "random.seed(1215151)\n",
    "print (random.random()) # same seed, same number!\n",
    "random.seed(1215151)\n",
    "print (random.random()) # same seed, same number!\n",
    "random.seed(5298496496)\n",
    "print (random.random()) # different seed, different number!\n",
    "random.seed(5298496496)\n",
    "print (random.random()) # repeat!\n",
    "\n",
    "print (random.random()) # no seed, purely random number\n",
    "print (random.random()) # no seed, purely random number\n",
    "\n",
    "np.random.seed(4198494)\n",
    "print(np.random.rand(3,2))\n",
    "np.random.seed(4198494)\n",
    "print(np.random.rand(3,2))\n",
    "# now without seed, most probaby it will be different!\n",
    "print(np.random.rand(3,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Important Note on Random Number/Vector Generation <a name=\"ImportantNoteonRandomNumberVectorGeneration\"></a>\n",
    "\n",
    "**Each group has to use a different number seed (which is an arbitrary number as illustrated above) and groups cannot share seeds. The pseudo-randomness is used here to create diversity. Otherwise, if groups use the same seed, you would lose points in assessment**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Economic Dispatch in Power Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem is formulated as\n",
    "$$ \\min_P \\sum_{i=1}^N c_i P_i $$\n",
    "$$\\text{subject to } P_{i,max} \\geq P_i \\geq 0, \\; \\forall i, \\text{ and } \\sum_{i=1}^N P_i = P_{demand} $$\n",
    "\n",
    "Here, $P_1,\\ldots P_N$ are the power generated by Generators $1,\\ldots,N$, $c_i$ is the per-unit generation cost of the i-_th_ generator, and $P_{demand}$ is the instantaneous power demand that needs to be satisfied by aggregate generation. More complex formulations take into account transmission, generator ramp-up and down constraints, and reactive power among other things."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question: Economic Dispatch \n",
    "\n",
    "Let us get inspired from generation in Victoria with $N=12$ biggest generators that have more than 200MW capacity. Choose their maximum generation randomly or from the Victoria generator report if you wish to be more realistic. Generate a random cost vector $c$ varying between $10-50$ AUD per MWh (use a group-specific random seed). _(Optionally, you can search and find how much different generation types cost if you are interested and add a bit of random noise to it)._ Let the demand be $P_{demand}=5000MW$. \n",
    "\n",
    "Solve this simplified [economic dispatch](https://en.wikipedia.org/wiki/Economic_dispatch) problem defined above. The resulting [merit order](https://en.wikipedia.org/wiki/Merit_order) is the generation that would have been if there was no NEM (electricity market).\n",
    "\n",
    "\n",
    "\n",
    "1. Solve the problem using *Pyomo*.\n",
    "2. What type of an optimisation problem is this? Briefly explain.\n",
    "\n",
    "> Further information: you can find more about Australian wholesale electricity market and generation at https://www.aemo.com.au/ See also this [NEM overview introductory document (right click to download)](./files/National_Electricity_Market_Fact_Sheet.pdf) and the [Victoria generator report as of January 2019](files/Generation_Information_VIC_January_2019.xlsx).\n",
    "\n",
    "**Note:** *if you are in the minority of people who have problem installing pyomo, then you can use scipy or even Matlab.*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer as text here**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Answer as code here '''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Waterfilling in Communications\n",
    "\n",
    "_by Robert Gowers, Roger Hill, Sami Al-Izzi, Timothy Pollington and Keith Briggs.\n",
    "From the book by Boyd and Vandenberghe, Convex Optimization, Example 5.2 page 245._\n",
    "\n",
    "$$\\min_{x} \\sum_{i=1}^N -\\log(\\alpha_i + x_i)$$ \n",
    "\n",
    "$$\\text{subject to } x_i \\geq 0, \\; \\forall i, \\text{ and } \\sum_{i=1}^N x_i = P $$\n",
    "\n",
    "This problem arises in information/communication theory, in allocating power to a\n",
    "set of $n$ communication channels. The variable $x_i$ represents the transmitter power\n",
    "allocated to the _i-th_ channel, and $\\log(\\alpha_i + x_i)$ gives the capacity or communication rate of the channel, where $\\alpha_i>0$ represents the floor above the baseline at which power can be added to the channel. The problem is to allocate a total power of one to the channels,\n",
    "in order to maximize the total communication rate.\n",
    "\n",
    "This can be solved using a classic [water filling algorithm](https://en.wikipedia.org/wiki/Water_filling_algorithm). \n",
    "\n",
    "![Waterfilling](img/waterfill.jpg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Question \n",
    "\n",
    "1. Is the problem in the Example above convex? Formally explain/argue why or why not. What does this imply regarding the solution? \n",
    "2. Solve the problem above for $N=8$ and a randomly chosen $\\alpha$ vector (use a group-specific random seed). You *can* use Pyomo for this. Cross-check your answer with another software (package), e.g. Matlab or Scipy. \n",
    "3. Write the Lagrangian, KKT conditions, and find numerically the Lagrange multipliers associated with the solution (using the software package/function). Which constraints are active? Explain and discuss briefly. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer as text here**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Answer as code here '''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Power Control in Wireless Communication"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " *Adapted from Boyd, Kim, Vandenberghe, and Hassibi,* \"[A Tutorial on Geometric Programming](https://web.stanford.edu/~boyd/papers/pdf/gp_tutorial.pdf).\"\n",
    "\n",
    "The [power control problem in wireless communications](http://winlab.rutgers.edu/~narayan/PAPERS/PC%20for%20Wireless%20Data.pdf) aims to minimise the total transmitter power available across $N$ trasmitters while concurrently achieving good (or a pre-defined minimum) performance. \n",
    "\n",
    "The technical setup is as follows. Each transmitter $i$ transmits with a power level $P_i$ bounded below and above by a minimum and maximum level. The power of the signal received from transmitter $j$ at receiver $i$ is $G_{ij} P_{j}$, where $G_{ij} > 0$ represents the path gain (often loss) from transmitter $j$ to receiver $i$. The signal power at the intended receiver $i$ is $G_{ii} P_i$, and the interference power at receiver $i$ from other transmitters is given by $\\sum_{k \\neq i} G_{ik}P_k$. The (background) noise power at receiver $i$ is $\\sigma_i$. Thus, the _Signal to Interference and  Noise Ratio (SINR)_ of the $i$th receiver-transmitter pair is\n",
    "\n",
    "$$ S_i = \\frac{G_{ii}P_i}{\\sum_{k \\neq i} G_{ik}P_k + \\sigma_i }. $$\n",
    "\n",
    "The minimum SINR represents a performance lower bound for this system, $S^{\\text min}$. \n",
    "\n",
    "The resulting optimisation problem is formulated as\n",
    "\n",
    "$$\n",
    "\\begin{array}{ll}\n",
    "\\min_{P} & \\sum_{i=1}^N P_i \\\\\n",
    "\\text{subject to} & P^{min} \\leq P_i \\leq P^{max}, \\; \\forall i \\\\\n",
    "& \\dfrac{G_{ii}P_i}{\\sigma_i + \\sum_{k \\neq i} G_{ik}P_k} \\geq S^{min} , \\; \\forall i \\\\\n",
    "\\end{array}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question: Wireless Power Control \n",
    "\n",
    "Let $N=10$, $P^{min}=0.1$, $P^{max}=5$, $\\sigma=0.2$ (same for all). Create a random path loss matrix $G$, where off-diagonal elements are between $0.1$ and $0.9$ and the diagonal elements are equal to $1$. \n",
    "\n",
    "1. Write down the Langrangian and KKT conditions of this problem.\n",
    "2. Solve the problem first with $S^{min}=0$ using *Pyomo*. Plot the power levels and SINRs that you obtain. \n",
    "3. What happens if you choose an $S^{min}$ that is larger? Solve the problem again and document your results. What happens if you choose a very large $S^{min}$? Observe and comment. \n",
    "\n",
    "**Note:** *if you are in the minority of people who have problem installing pyomo, then you can use scipy or even Matlab.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer as text here**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Answer as code here '''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part II - Machine Learning (ML) \n",
    "\n",
    "## Topics Covered in Part 2<a name=\"TopicsCovered\"></a>\n",
    "* ML basics and definitions, supervised and unsupervised learning\n",
    "* Linear regression, model selection, and regularisation\n",
    "* Clustering, k-means, and Gaussian Mixture Models\n",
    "* _Remaining topics will be covered in Workshop 2!_\n",
    "\n",
    "![Topic_mindmap](img/WS1-Part_2-ML_topics.png)\n",
    "\n",
    "## Part 2 Notes  <a name=\"TopicNotes\"></a>\n",
    "\n",
    "Optimisation is widely used in engineering (practice and research) today. That was not always so. I expect that in the future machine learning will be as prevalently used in engineering as optimisation is used today. The arguments in favour of it are (a) increasingly more powerful computing (b) lots of data (c) decreasing storage and computing costs. Machine learning benefits substantially from these trends. We will hopefully see together how engineering world will evolve in this century.\n",
    "\n",
    "In this workshop, you will learn how to solve machine learning problems in practice and apply common algorithms to various data sets. Doing this yourself will give you a chance to connect theoretical knowledge and practical usage. We will start with simple, easy-to-visualise (2D) data sets so that concepts become clear. More interesting problems and data will be posed as open-ended (and optional) problems.\n",
    "\n",
    "You will also familiarise yourself with machine learning libraries of Python, which is the de-facto language for ML these days. Still, the tools and data are chosen completely for educational reasons (simplicity, accessibility, cost). There are and will be better ML frameworks and more complex data sets but it is not realistic to cover all. Due to time limitations, we unfortunately do not focus on a big topic in this workshop and subject: [data science](https://study.unimelb.edu.au/find/courses/graduate/master-of-data-science/what-will-i-study/). You should not get the wrong impression from the nice, cleaned-up data sets you are given in this workshop. In real life, data is messy and more than half of data science is about preparing data itself. \n",
    "\n",
    "> In the future, you should consider learning additional ML software packages and libraries. Finding the right tool for the right job is an important skill obtained through knowledge and experience. I would also recommend learning more about data preparation and analysis. The popular [Pandas](https://pandas.pydata.org/) library, which we briefly use, makes a good starting point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 Objectives <a name=\"Objectives\"></a>\n",
    "* Use these problems as a motivation to learn the fundamentals of machine learning covered in lectures.\n",
    "* Gain hands-on experience with basic machine learning paradigms.\n",
    "* Familiarise yourself with some of the practical software tools used for machine learning.\n",
    "* Solve basic machine learning problems using Python Scipy and Scikit-learn.\n",
    "* Connect theoretical knowledge and practical usage by doing it yourself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Linear Regression, Overfitting, and Regularisation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Using curve fitting to model Diode characteristics\n",
    "\n",
    "![Diode characteristic](img/diode.png)\n",
    "\n",
    "The diagram above shows the I-V curve of a [diode](https://en.wikipedia.org/wiki/Diode) widely used in electronic circuits, see [1N4001-D spec sheet (right click to download)](1N4001-D.pdf).\n",
    "\n",
    "We can use regression to model the I-V curve of this diode at 25$^{\\circ}$C.\n",
    "\n",
    "Using the nice tool, [WebPlotDigitizer](https://automeris.io/WebPlotDigitizer/), a small and clean data set is generated and stored in [csv format](https://en.wikipedia.org/wiki/Comma-separated_values). We now use the famous [pandas library](https://pandas.pydata.org/) to read the *csv* file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-25T02:45:10.911242Z",
     "start_time": "2019-03-25T02:45:10.388090Z"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import optimize\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "\n",
    "dataset=pd.read_csv('files/diode_dataset.csv', names=['Vf', 'If'])\n",
    "# Note that if you don't put names to csv or into the function as above, \n",
    "# pandas ignores the first row in calculations!\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-25T02:45:10.989815Z",
     "start_time": "2019-03-25T02:45:10.965563Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(dataset.values[:,0], dataset.values[:,1])\n",
    "plt.xlabel('Voltage, V')\n",
    "plt.ylabel('Current, I')\n",
    "plt.title('Diode I-V')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Note that the figure above is convex but the one above was looking concave! Can you see why?_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question: Regression\n",
    "\n",
    "Now let's consider the linear model $I=f(V)=a+bV$, for $a, b \\in \\mathbb{R}$ for simplicity.\n",
    "\n",
    "We can find the best $(a, b)$ that minimises the error between the $N$ data points $(I_j, V_j)$ and this linear model by solving the optimisation problem \n",
    "\n",
    "$$\\min_{a, b} \\sum_{j=1}^N (I_j- (a + b V_j))^2 $$\n",
    "\n",
    "This is equivalent *in spirit* to what the machine learning libraries such as *scikit-learn (sklearn)* do to solve this problem! As you will repeatedly see, there is a deep and close relationship between optimisation and many learning methods.\n",
    "\n",
    "1. Find the optimal $a, b$ pair by solving the unconstrained optimisation problem above using *pyomo*. Using the formula and the parameters, $a, b$, you derived, plot the linear I-V curve with the additional constraint $I \\geq 0$ or $\\max(I, 0)$.\n",
    "2. First, fit a [linear model](https://scikit-learn.org/stable/modules/linear_model.html) using \"linear_model.LinearRegression()\". Plot the result, find the coefficients, and calculate the mean squared error (MSE).\n",
    "3. Next, fit a [polynomial model](https://scikit-learn.org/stable/modules/linear_model.html#polynomial-regression-extending-linear-models-with-basis-functions) of second degree, i.e. a quadratic model. Plot the model, find the coefficients, and calculate MSE. Interpret and discuss the results.\n",
    "4. Add a regularisation term, i.e use [ridge regression](https://scikit-learn.org/stable/modules/linear_model.html#ridge-regression). Do you observe an improvement? Discuss.\n",
    "5. Try a couple of higher order models (e.g. 4, 6) without regularisation, and provide the results as before. What do you observe when you compare the coefficients? Find validation and training errors for the models and discuss/interpret your results.\n",
    "\n",
    "#### Hints <a name=\"Hints\"></a>\n",
    "1. you will need to use [pipelines](https://scikit-learn.org/stable/modules/compose.html#pipeline). To access coefficients use `model.named_steps['linearregression'].coef_` or `model.named_steps['ridge'].coef_`\n",
    "2. the `train_test_split` function provides a very convenient way of shuffling the data and dividing it into  training and test sets (3:1 default ratio), see https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-25T02:45:20.091506Z",
     "start_time": "2019-03-25T02:45:19.845232Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# full data in correct form for sklearn\n",
    "Vfulldata = np.array(dataset.values[:,0]).reshape(-1,1) # reshape needed for sklearn functions\n",
    "Ifulldata = np.array(dataset.values[:,1]).reshape(-1,1)\n",
    "\n",
    "# split into training and test sets\n",
    "Vtrain, Vtest, Itrain, Itest = train_test_split(Vfulldata, Ifulldata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer as text here**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "''' Answer as code here '''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering and Gaussian Mixtures "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unsupervised learning is all about data. We will use first the famous two moon data set to practice a little bit and digest some of the fundemental concepts. Since two moons data inherently has two clusters (each moon as a cluster), we can use this as a [ground truth](https://en.wikipedia.org/wiki/Ground_truth). In most real problems, we don't have this luxury of having the ground truth at hand! \n",
    "\n",
    "**Note** that Scikit Learn does not have its own global random state but uses the [numpy random state](https://docs.scipy.org/doc/numpy-1.15.1/reference/generated/numpy.random.seed.html#numpy.random.seed) instead. See the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-25T02:45:35.973271Z",
     "start_time": "2019-03-25T02:45:35.902585Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn import cluster, datasets, mixture\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn import decomposition\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn import metrics\n",
    "\n",
    "# Set a random seed as you did in optimisation workshop by uncommenting the line below!\n",
    "#np.random.seed(Put here a group-specific number!)\n",
    "\n",
    "noisy_moons = datasets.make_moons(n_samples=200, noise=0.05)\n",
    "X = noisy_moons[0] # data points\n",
    "y = noisy_moons[1] # 0, 1 labels of class, 50 each - giving us the ground truth\n",
    "\n",
    "order_ind = np.argsort(y) # order labels, 50 each class\n",
    "X1 = X[order_ind[0:100]]   # class 1\n",
    "X2 = X[order_ind[101:200]] # class 2\n",
    "\n",
    "# Plot data\n",
    "plt.figure()\n",
    "plt.scatter(X1[:,0], X1[:,1], color='black')\n",
    "plt.scatter(X2[:,0], X2[:,1], color='red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question: K-means clustering \n",
    "1. Use sklearn's k-means clustering algorithm to divide the two moon data given above ($X$) into two clusters. Plot the result and show the cluster centres that you found.\n",
    "2. Experiment with different starting points (`init='random'`) and number of clusters, e.g. 3, 4, 5. Write your observations and interpret them using your theoretical knowledge from lectures and books."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Answer as code here '''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer as text here**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question: GMMs as density estimators \n",
    "\n",
    "1. Now use a Gaussian Mixture Model (GMM) for clustering the same two moon data. Try two clusters and plot your results. GMMs also provides you probabilities (of a sample belonging to a cluster). Print those of a few samples.\n",
    "2. Increase the number of components of your GMM model. What do you observe? Use a metric to choose the number of components in a principled way. *Hint: check [BIC](https://en.wikipedia.org/wiki/Bayesian_information_criterion) or [AIC](https://en.wikipedia.org/wiki/Akaike_information_criterion)*\n",
    "3. It is maybe better to use GMM as a **generative model**! Generate 200 brand new samples from a trained GMM with your choice of parameters and plot your results. Discuss your findings!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Answer as code here '''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer as text here**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='red'> Workshop Assessment Instructions </font> \n",
    "\n",
    "_You should complete the workshop tasks and answer the questions within the allocated session!_ **Submission deadline is usually end of the last week of the workshop. Please check Canvas for the exact deadline!**\n",
    "\n",
    "It is **mandatory to follow all of the submissions guidelines** given below. _Don't forget the Report submission information on top of this notebook!_\n",
    "\n",
    "1. The completed Jupyter notebook and its Pdf version (you can simply print-preview and then print as pdf from within your browser) should be uploaded to the right place in Canvas. _It is your responsibility to follow the announcements!_ **Late submissions will be penalised (up to 100% of the total mark depending on delay amount)!**\n",
    "2. Filename should be “ELEN90088 Workshop **W: StudentID1-StudentID2** of session **Day-Time**\", where **W** refers to the workshop number, **StudentID1-StudentID2** are your student numbers, **Day-Time** is your session day and time, e.g. *Tue-14*.\n",
    "3. Answers to questions, simulation results and diagrams should be included in the Jupyter notebook as text, code, plots. *If you don't know latex, you can write formulas/text to a paper by hand, scan it and then include as image within Markdown cells.*\n",
    "4. Please submit your report individually. Partners can submit the same report. \n",
    "\n",
    "## Workshop Marking \n",
    "\n",
    "* **Each workshop has 15 points corresponding to 15% of the total subject mark inclusive individual oral examination.** You can find the detailed rubrics on Canvas.\n",
    "* Individual oral quizzes will be scheduled within the next two weeks following the report submission. They will be during workshop hours. Therefore, it is important that you attend the workshops!\n",
    "* The individual oral examination will assess your answers to workshop questions, what you have done in that workshop, and your knowledge of the subject material in association with the workshop.\n",
    " \n",
    "### Additional guidelines for your programs: \n",
    "\n",
    "* Write modular code using functions. \n",
    "* Properly indent your code. But Python forces you do that anyway ;)\n",
    "* Heavily comment the code to describe your implementation and to show your understanding. No comments, no credit!\n",
    "* Make the code your own! It is encouraged to find and get inspired by online examples but you should exactly understand, modify as needed, and explain your code via comments. If you resort to blind copy/paste, you will certainly not do well in the individual oral quizzes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<font color='red'>Report Submission Information (must be completed before submitting report!)</font>**\n",
    "\n",
    "* Student 1 Full Name and Number: \n",
    "* Student 2 Full Name and Number: \n",
    "* Workshop day: e.g., Wednesday\n",
    "* Workshop time: e.g., 12pm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "365.275px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
